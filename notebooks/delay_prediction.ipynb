{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict delays for every scheduled connection\n",
    "\n",
    "In this notebook, we provide schedule connections with an uncertainty parameter for prediciting possible delays. In other words, we map the static public transport network with the delay analysis in order to obtain a meaningful and accurate predictive model. The notebook is structured as follows: \n",
    "\n",
    "*   **[Start Spark](#spark)** \n",
    "*   **[Merge connections and probabilities](#merge)**  \n",
    "*   **[Get predictive model](#predictive)** \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'spark'></a>\n",
    "### 1. Start Spark\n",
    "\n",
    "We will be using a Spark Session for performing different transformations and actions on dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.app.name\": \"datavirus_delay_prediction\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>8421</td><td>application_1589299642358_2953</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_2953/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_2953_01_000001/ebouille\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fa6ab43e290>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'merge'></a>\n",
    "### 2. Merge connections and probabilities\n",
    "\n",
    "We load our two dataframes: the \"ideal\" schedule for the connections and the one containing the delay probabilities. We are able to join them thanks to the previously performed transformations. However, we have to be careful, there is not a perfect 1-to-1 correspondence between the connections data and the probability information. There is no probability data available for some of the connections. For those connections we use the transport probability data which we created in the [delay_analysis](./delay_analysis.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id_connections = spark.read.orc('/user/datavirus/id_connections.orc').repartition(150, 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probability = spark.read.orc('/user/datavirus/probability.orc').repartition(150, 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# join the two dataframes\n",
    "probability_connections = (\n",
    "    id_connections\n",
    "    .join(probability, ['id', 'station_id', 'arrival_time_minute'], how='left_outer')\n",
    "    .select(\n",
    "        id_connections.stop_sequence,\n",
    "        id_connections.route_type,\n",
    "        id_connections.arrival_time_hour,\n",
    "        id_connections.produkt_id,\n",
    "        id_connections.start_id,\n",
    "        id_connections.start_time,\n",
    "        id_connections.trip_id,\n",
    "        probability.transport_type,\n",
    "        probability.line_text,\n",
    "        id_connections.stop_time,\n",
    "        id_connections.stop_id,\n",
    "        probability.delay_probability,\n",
    "        probability.delay_parameter\n",
    "    )\n",
    ")\n",
    "\n",
    "probability_connections.write.format('orc').save('/user/datavirus/probability_connections.orc', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \"back-up\" data for trips without corresponding\n",
    "# probability data\n",
    "transport_probability = spark.read.orc('/user/datavirus/transport_probability.orc').alias('transport_probability')\n",
    "transport_probability = (\n",
    "    transport_probability\n",
    "    .withColumn('arrival_time_hour', transport_probability.ankunftszeit_hour)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'predictive'></a>\n",
    "### 3. Get predictive model\n",
    "\n",
    "At last, we can obtain the final connection dataframe with the probability parameters included. This will help us predict the future delay of any connection incorporated in our transport network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# add transport_probability information\n",
    "# for missing data\n",
    "connections = (\n",
    "    probability_connections\n",
    "    .join(transport_probability, ['produkt_id', 'arrival_time_hour'])\n",
    "    .select(\n",
    "        probability_connections.stop_sequence,\n",
    "        probability_connections.route_type,\n",
    "        probability_connections.start_id,\n",
    "        probability_connections.start_time,\n",
    "        probability_connections.trip_id,\n",
    "        probability_connections.produkt_id.alias('transport_type'),\n",
    "        probability_connections.line_text,\n",
    "        probability_connections.stop_time,\n",
    "        probability_connections.stop_id,\n",
    "        f.when(\n",
    "            f.col('probability_connections.delay_probability').isNotNull(),\n",
    "            f.col('probability_connections.delay_probability')\n",
    "        ).otherwise(\n",
    "            f.col('transport_probability.transport_delay_probability')\n",
    "        ).alias('delay_probability'),\n",
    "        f.when(f.col('probability_connections.delay_parameter').isNotNull(),\n",
    "              f.col('probability_connections.delay_parameter')\n",
    "        ).otherwise(\n",
    "            f.col('transport_probability.transport_delay_parameter')\n",
    "        ).alias('delay_parameter')\n",
    "    )\n",
    "    .orderBy([probability_connections.stop_time.desc(), probability_connections.stop_sequence.desc()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "connections.write.format('csv').save('/user/datavirus/connections.csv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------+----------+-----------------------+--------------+---------+---------+-------+------------------+--------------------+\n",
      "|stop_sequence|route_type|start_id|start_time|trip_id                |transport_type|line_text|stop_time|stop_id|delay_probability |delay_parameter     |\n",
      "+-------------+----------+--------+----------+-----------------------+--------------+---------+---------+-------+------------------+--------------------+\n",
      "|2            |700       |8502209 |10:05:00  |9.TA.30-170-Y-j19-1.1.H|bus           |null     |10:05:00 |8502209|0.9193866847990115|0.011589134124321832|\n",
      "|2            |700       |8502771 |10:05:00  |392.TA.26-235-j19-1.5.R|bus           |235      |10:05:00 |8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2            |700       |8502771 |10:05:00  |493.TA.26-235-j19-1.5.R|bus           |235      |10:05:00 |8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2            |700       |8502771 |10:05:00  |360.TA.26-235-j19-1.5.R|bus           |235      |10:05:00 |8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2            |700       |8502771 |10:05:00  |345.TA.26-235-j19-1.5.R|bus           |235      |10:05:00 |8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2            |700       |8502771 |10:05:00  |439.TA.26-235-j19-1.5.R|bus           |235      |10:05:00 |8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2            |700       |8502771 |10:05:00  |224.TA.26-235-j19-1.4.R|bus           |235      |10:05:00 |8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2            |700       |8502771 |10:05:00  |390.TA.26-235-j19-1.5.R|bus           |235      |10:05:00 |8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2            |700       |8502771 |10:05:00  |410.TA.26-235-j19-1.5.R|bus           |235      |10:05:00 |8502771|0.9159136766383016|0.013732196842403623|\n",
      "|2            |700       |8502771 |10:05:00  |450.TA.26-235-j19-1.5.R|bus           |235      |10:05:00 |8502771|0.9159136766383016|0.013732196842403623|\n",
      "+-------------+----------+--------+----------+-----------------------+--------------+---------+---------+-------+------------------+--------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "connections.show(10, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
